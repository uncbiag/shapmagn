"""
PointPWC-Net model and losses
Author: Wenxuan Wu
Date: May 2020
"""
from functools import partial
import torch.nn as nn
import torch
from shapmagn.modules_reg.networks.pointconv_util import PointConv, PointConvD, UpsampleFlow3
from shapmagn.modules_reg.networks.pointconv_util import Conv1d
from shapmagn.utils.utils import shrink_by_factor
scale = 1.0


class PointPWC_Landmark_MultiFlow(nn.Module):
    def __init__(self, input_channel=3, initial_input_radius=1.,first_sampling_npoints=2048, predict_at_low_resl=False,param_shrink_factor=1,output_channels=8,use_aniso_kernel=False):
        super(PointPWC_Landmark_MultiFlow, self).__init__()

        feat_nei = 16
        sbf = partial(shrink_by_factor, factor=param_shrink_factor)
        self.predict_at_low_resl = predict_at_low_resl
        self.scale = 1
        self.res_scale = 0.1
        self.heatmap_threshold = 0.9
        # l0: 8192
        self.level0 = Conv1d(input_channel, sbf(32))
        self.level0_1 = Conv1d(sbf(32), sbf(32))
        self.level0_2 = Conv1d(sbf(32), sbf(64))


        # l1: 2048
        self.level1 = PointConvD(first_sampling_npoints, feat_nei, sbf(64)+3, sbf(64), use_aniso_kernel=use_aniso_kernel, cov_sigma_scale=initial_input_radius*20,aniso_kernel_scale=initial_input_radius*80)
        self.level1_0 = Conv1d(sbf(64), sbf(64))
        self.level1_1 = Conv1d(sbf(64), sbf(128))
        self.level1_2 = PointConvD(int(first_sampling_npoints),feat_nei, sbf(64+32)+3, sbf(64), group_all=True)
        self.heatmap1_re = PointConv(9, sbf(64)+3, sbf(64), bn=True, use_leaky=True)
        self.heatmap1_conv = Conv1d( sbf(64), output_channels)


        # l2: 512
        self.level2 = PointConvD(int(first_sampling_npoints/4), feat_nei, sbf(128) + 3, sbf(128))
        self.level2_0 = Conv1d(sbf(128), sbf(128))
        self.level2_1 = Conv1d(sbf(128), sbf(256))
        self.level2_2 = PointConvD(int(first_sampling_npoints/4),feat_nei, sbf(128+64)+3, sbf(128), group_all=True)
        self.heatmap2_re = PointConv(9, sbf(128)+3, sbf(128), bn=True, use_leaky=True)
        self.heatmap2_conv = Conv1d( sbf(128), output_channels)


        # l3: 256
        self.level3 = PointConvD(int(first_sampling_npoints/8), feat_nei, sbf(256) + 3, sbf(256))
        self.level3_0 = Conv1d(sbf(256), sbf(256))
        self.level3_1 = Conv1d(sbf(256), sbf(512))
        self.level3_2 = PointConvD(int(first_sampling_npoints/8),feat_nei, sbf(256+64)+3, sbf(256), group_all=True)
        self.heatmap3 =PointConv(9, sbf(256)+3, sbf(256), bn=True, use_leaky=True)
        self.heatmap3_conv = Conv1d(sbf(256), output_channels)

        # l4: 64
        self.level4 = PointConvD(int(first_sampling_npoints/32), feat_nei, sbf(512) + 3, sbf(256))
        self.level4_0 =PointConvD(int(first_sampling_npoints/32), feat_nei, sbf(256) + 3, sbf(256))


        # deconv
        self.deconv4_3 = Conv1d(sbf(256), sbf(64))
        self.deconv3_2 = Conv1d(sbf(256), sbf(64))
        self.deconv2_1 = Conv1d(sbf(128), sbf(32))

        self.heatmap0_re = PointConv(9, sbf(64)+3, sbf(64), bn=True, use_leaky=True)
        self.heatmap0_conv = Conv1d(sbf(64), output_channels)


        # upsample
        self.upsample = UpsampleFlow3(-1)


    def normalize(self, heatmap, points, given_bound=None):
        B, L = heatmap.shape[:2]
        if given_bound:
            hm_min, hm_max = given_bound
        else:
            hm_min = heatmap.min(2)[0].view(B, L, 1)
            hm_max = heatmap.max(2)[0].view(B, L, 1)
        heatmap = (heatmap - hm_min) / (hm_max - hm_min)
        heatmap_cp = heatmap.clone()
        heatmap_cp[heatmap < self.heatmap_threshold] = 0
        heatmap = heatmap_cp / (heatmap_cp.sum(-1, keepdim=True) + 1e-9)
        landmarks = torch.einsum("bln,bdn -> bdl",heatmap, points)
        return landmarks

    def forward(self, xyz1, color1):
        # xyz1, xyz2: B, N, 3
        # color1, color2: B, N, 3

        # l0
        _scale = 2
        scaler = lambda x: self.scale *(_scale**x)
        pc1_l0 = xyz1.permute(0, 2, 1)
        color1 = color1.permute(0, 2, 1)  # B 3 N
        feat1_l0 = self.level0(color1)
        feat1_l0 = self.level0_1(feat1_l0)
        feat1_l0_1 = self.level0_2(feat1_l0)


        # l1
        pc1_l1, feat1_l1, fps_pc1_l1 = self.level1(pc1_l0, feat1_l0_1)
        feat1_l1_2 = self.level1_0(feat1_l1)
        feat1_l1_2 = self.level1_1(feat1_l1_2)


        # l2
        pc1_l2, feat1_l2, fps_pc1_l2 = self.level2(pc1_l1, feat1_l1_2)
        feat1_l2_3 = self.level2_0(feat1_l2)
        feat1_l2_3 = self.level2_1(feat1_l2_3)


        # l3
        pc1_l3, feat1_l3, fps_pc1_l3 = self.level3(pc1_l2, feat1_l2_3)
        feat1_l3_4 = self.level3_0(feat1_l3)
        feat1_l3_4 = self.level3_1(feat1_l3_4)


        # l4
        pc1_l4, feat1_l4, _ = self.level4(pc1_l3, feat1_l3_4)
        _, feat1_l4_3, _ = self.level4_0(pc1_l4, feat1_l4)



        # l3
        feat1_l4_3 = self.upsample(pc1_l3, pc1_l4, feat1_l4, resol_factor=scaler(7))
        feat1_l4_3 = self.deconv4_3(feat1_l4_3)
        c_feat1_l3 = torch.cat([feat1_l3, feat1_l4_3], dim=1)
        _, c_feat1_l3, _ = self.level3_2 (pc1_l3, c_feat1_l3)
        heatmap3 = self.heatmap3_conv(self.heatmap3(pc1_l3,c_feat1_l3))
        landmark3 = self.normalize(heatmap3,pc1_l3)



        # l2
        heatmap3_up = self.upsample(pc1_l2, pc1_l3, heatmap3, resol_factor=scaler(5))
        feat1_l3_2 = self.upsample(pc1_l2, pc1_l3, c_feat1_l3, resol_factor=scaler(5))
        feat1_l3_2 = self.deconv3_2(feat1_l3_2)
        c_feat1_l2 = torch.cat([feat1_l2, feat1_l3_2], dim=1)
        _, c_feat1_l2, _ = self.level2_2 (pc1_l2, c_feat1_l2)
        heatmap_re2 = self.heatmap2_conv(self.heatmap2_re(pc1_l2,c_feat1_l2))*self.res_scale
        heatmap2 = heatmap3_up+heatmap_re2
        landmark2 = self.normalize(heatmap2,pc1_l2)



        # l1
        heatmap2_up = self.upsample(pc1_l1, pc1_l2, heatmap2, resol_factor=scaler(5))
        feat1_l2_1 = self.upsample(pc1_l1, pc1_l2, c_feat1_l2, resol_factor=scaler(5))
        feat1_l2_1 = self.deconv2_1(feat1_l2_1)
        c_feat1_l1 = torch.cat([feat1_l1, feat1_l2_1], dim=1)
        _, c_feat1_l1, _ = self.level1_2(pc1_l1, c_feat1_l1)
        heatmap_re1 = self.heatmap1_conv(self.heatmap1_re(pc1_l1, c_feat1_l1))*self.res_scale
        heatmap1 = heatmap2_up+heatmap_re1
        landmark1 = self.normalize(heatmap1,pc1_l1)
        tp = lambda x: x.transpose(2, 1).contiguous()

        if not self.predict_at_low_resl:
            heatmap1_up= self.upsample(pc1_l0, pc1_l1, heatmap1, resol_factor=scaler(5))
            feat1_l1_0 = self.upsample(pc1_l0, pc1_l1, c_feat1_l1, resol_factor=scaler(5))
            heatmap_re0 = self.heatmap0_conv(self.heatmap0_re(pc1_l0,feat1_l1_0))*self.res_scale
            heatmap0 = heatmap1_up + heatmap_re0
            landmark0 = self.normalize(heatmap0,pc1_l0)
            landmarks = [tp(landmark0), tp(landmark1), tp(landmark2), tp(landmark3)]
            control_points= [tp(pc1_l0), tp(pc1_l1), tp(pc1_l2), tp(pc1_l3)]
            heatmaps = [heatmap0, heatmap1, heatmap2, heatmap3]
            return tp(landmark0), {"landmarks": landmarks, "control_points":control_points, "heatmaps":heatmaps, "weights":[1.6,0.8,0.4,0.2], "predict_at_low_resl": self.predict_at_low_resl, "multi_scale":True}  # BxCxN,  BxNxD
        else:
            landmarks = [tp(landmark1), tp(landmark2), tp(landmark3)]
            control_points= [tp(pc1_l1), tp(pc1_l2), tp(pc1_l3)]
            heatmaps = [heatmap1, heatmap2, heatmap3]
            return tp(landmark1), {"landmarks": landmarks, "control_points":control_points,"heatmaps":heatmaps,"weights":[0.8,0.4,0.2], "predict_at_low_resl": self.predict_at_low_resl, "multi_scale":True}  # BxCxN,  BxNxD






